{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integral-break",
   "metadata": {},
   "source": [
    "# Interview experiments for blenderbot\n",
    "\n",
    "In order to get Freja to act as an inteviewer we have to constrain her. Blenderbot(the model) is trained as an open-domain chat bot, which makes her very explorative when it comes to asking questions.\n",
    "\n",
    "#### Approach:\n",
    "- Hard code a set of interview questions\n",
    "- System for determining when it's time to move on to next question\n",
    "- Experiment with the dialogue context she gets at each pass of the conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-shanghai",
   "metadata": {},
   "source": [
    "# Blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "from torch import no_grad\n",
    "mname = 'facebook/blenderbot-1B-distill' # options: 'facebook/blenderbot_small-90M' , 'facebook/blenderbot-400M-distill' ,'facebook/blenderbot-3B'\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-toddler",
   "metadata": {},
   "source": [
    "## BlenderConversation is a class for storing the conversation with blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlenderConversation:\n",
    "    \n",
    "    def __init__(self,lang,description='No description'):\n",
    "        self.lang = lang\n",
    "        self.description = description\n",
    "        self.bot_text = []\n",
    "        self.user_text = []\n",
    "        self.user_turn = True\n",
    "        \n",
    "        \n",
    "    def add_user_text(self,text):\n",
    "        if self.user_turn:\n",
    "            self.user_text.append(text)\n",
    "            self.user_turn = False\n",
    "        else:\n",
    "            raise ValueError(\"It's the bot's turn to add a reply to the conversation\")\n",
    "        return\n",
    "    \n",
    "    def add_bot_text(self,text):\n",
    "        if not self.user_turn:\n",
    "            self.bot_text.append(text)\n",
    "            self.user_turn = True\n",
    "        else:\n",
    "            raise ValueError(\"It's the user's turn to add an input to the conversation\")\n",
    "        return\n",
    "    \n",
    "    def pop(self):\n",
    "        if self.user_turn:\n",
    "            self.bot_text.pop()\n",
    "            self.user_turn = False\n",
    "        else:\n",
    "            self.user_text.pop()\n",
    "            self.user_turn = True\n",
    "        return\n",
    "    \n",
    "    def get_bot_replies():\n",
    "        # TODO: Option to return string instead of list?\n",
    "        return self.bot_text\n",
    "    \n",
    "    def get_user_replies():\n",
    "        # TODO: Option to return string instead of list?\n",
    "        return self.user_text\n",
    "        \n",
    "    def get_dialogue_history(self,max_len=100):\n",
    "        # Returns string of the dialogue history with bot and user inputs separated with '\\n'\n",
    "        # max_len set to default 110 as model has max input length 128 and we want some space for new input \n",
    "        history = ''\n",
    "        tokens_left = max_len\n",
    "        if self.user_turn:\n",
    "            # Start backwards from bot_text\n",
    "            for i in reversed(range(len(self.user_text))):\n",
    "                bot_text = self.bot_text[i]\n",
    "                user_text = self.user_text[i]\n",
    "                nbr_tokens = len(tokenizer(bot_text)['input_ids'])  + len(tokenizer(user_text)['input_ids'])\n",
    "                if  nbr_tokens < tokens_left: # This is not fool proof as the model tokenizer tokenizes differently\n",
    "                    history = user_text + '\\n' + bot_text + '\\n' + history\n",
    "                    tokens_left -= (nbr_tokens + 2)\n",
    "                else:\n",
    "                    break\n",
    "                                \n",
    "        else:\n",
    "            # Start backwards from user_text\n",
    "            history = self.user_text[-1]\n",
    "            tokens_left -= len(tokenizer(history)['input_ids'])\n",
    "            for i in reversed(range(len(self.user_text)-1)):\n",
    "                bot_text = self.bot_text[i]\n",
    "                user_text = self.user_text[i]\n",
    "                nbr_tokens = len(bot_text.split()) + len(user_text.split())\n",
    "                if  nbr_tokens < tokens_left: # This is not fool proof as the model tokenizer tokenizes differently\n",
    "                    history = user_text + '\\n' + bot_text + '\\n' + history\n",
    "                    tokens_left -= (nbr_tokens + 2)\n",
    "                else:\n",
    "                    break\n",
    "        return history\n",
    "        \n",
    "    def to_txt(self,file='None'):\n",
    "        # Writes the dialogue to txt file in subdirectory\n",
    "        text = '####################################\\n' + 'Conversation description: ' + self.description + '\\n\\n'\n",
    "        if self.user_turn:\n",
    "            for i in range(len(self.user_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "        else:\n",
    "            for i in range(len(self.bot_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "            text = text + 'User>>> ' + self.user_text[-1]\n",
    "        \n",
    "        if file is None:\n",
    "            if self.lang == 'sv':\n",
    "                file = 'interview_sv.txt'\n",
    "            else:\n",
    "                file = 'interview_en.txt'\n",
    "        \n",
    "        text = text + '\\n\\n'\n",
    "        file_path = '02_interview_output/' + file\n",
    "        with open(file_path,'a') as f:\n",
    "            f.write(text)\n",
    "        return\n",
    "         \n",
    "    \n",
    "    def print_dialogue(self):\n",
    "        # Prints the dialogue \n",
    "        text = ''\n",
    "        if self.user_turn:\n",
    "            for i in range(len(self.user_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "        else:\n",
    "            for i in range(len(self.bot_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "            text = text + 'User>>> ' + self.user_text[-1]\n",
    "        print(text)\n",
    "        return\n",
    "\n",
    "\n",
    "def strip_token(line):\n",
    "    # Removes SOS and EOS tokens from blenderbot reply\n",
    "    line = line.replace('<s>','')\n",
    "    line = line.replace('</s>','')\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-ribbon",
   "metadata": {},
   "source": [
    "## Class for keeping track of the interview and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minimal-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Är det någonting som du vill fråga om detta arbetet?',\n",
       " 'Du har sökt jobbet som car mech. Vad är det som du tycker verkar vara roligt med detta arbetet?',\n",
       " 'Vad är din bästa erfarenhet från dina tidigare arbeten?',\n",
       " 'Om du ska arbeta som car mech så är det bra om du har erfarenhet från YY. Kan du berätta lite om du har sådan erfarenhet?',\n",
       " 'Vad gör att du skulle passa bra som car mech?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a place to store questions instead of having txt files for now\n",
    "import random\n",
    "questions = ['Du har sökt jobbet som {}. Vad är det som du tycker verkar vara roligt med detta arbetet?', \n",
    "                       'Om du ska arbeta som {} så är det bra om du har erfarenhet från YY. Kan du berätta lite om du har sådan erfarenhet?',\n",
    "                       'Vad är din bästa erfarenhet från dina tidigare arbeten?',\n",
    "                       'Vad gör att du skulle passa bra som {}?',\n",
    "                   \n",
    "             'Är det någonting som du vill fråga om detta arbetet?']\n",
    "format_question = [1,1,0,1,0]\n",
    "interview_questions= zip(questions,format_question)\n",
    "test = [question.format('car mech') if format_this else question for (question, format_this) in interview_questions]\n",
    "random.shuffle(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "manual-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "from torch import no_grad\n",
    "mname = 'facebook/blenderbot-1B-distill' # options: 'facebook/blenderbot_small-90M' , 'facebook/blenderbot-400M-distill' ,'facebook/blenderbot-3B'\n",
    "import random\n",
    "\n",
    "class InterviewWorld:\n",
    "# Class that keeps\n",
    "\n",
    "    def __init__(self,job,name,mname='facebook/blenderbot-1B-distill'):\n",
    "        # TODO: More sophisticated questions/greeting drawn from txt file(?) and formated with name and job\n",
    "        # TODO: init model and tokenizer from file\n",
    "        self.questions = [question.format(job) if format_this else question for (question, format_this) in interview_questions]\n",
    "        random.shuffle(self.questions)\n",
    "        self.greeting = 'Hej, och välkommen till din intervju. Hur står det till, {}?'.format(name)\n",
    "        self.context = '' # TODO, maybe a function that updates this as well\n",
    "        \n",
    "        self.job = job\n",
    "        self.human_name = name\n",
    "        self.model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "        self.tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n",
    "        self.model_name = mname.replace('facebook/','')\n",
    "        self.translator = Translator()\n",
    "        self.episode_done = False \n",
    "        self.stop_tokens = ['färdig', 'slut', 'hejdå', 'done'] # TODO: Snyggare lösning \n",
    "        self.max_replies = 2 # Maximum number of replies back and forth for each question\n",
    "        self.nbr_replies = 0\n",
    "        \n",
    "        desc = 'InterviewWorld\\t job: {}\\t name: {}\\t model: {}'.format(self.job,self.human_name,self.model_name)\n",
    "        self.conversation_sv = BlenderConversation(lang='sv',description=desc)\n",
    "        self.conversation_en = BlenderConversation(lang='en',description=desc)\n",
    "        \n",
    "        self.greet()\n",
    "        \n",
    "    def greet(self):\n",
    "        print(self.greeting)\n",
    "        return\n",
    "        \n",
    "    def start(self):\n",
    "        # TODO: Prompt the user to add name and job they're looking for?\n",
    "        return\n",
    "    \n",
    "    def chat(self,user_input):\n",
    "        observe(user_input)\n",
    "        act(user_input)\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def observe(self,user_input):\n",
    "        # TODO: Add spell check/grammar check here\n",
    "        # Observe the user input, translate and update internal states\n",
    "        # Check if user wants to quit/no questions left --> self.episode_done = True\n",
    "        \n",
    "        translated_input = self._sv_to_en(user_input)\n",
    "        self.conversation_sv.add_user_text(user_input)\n",
    "        self.conversation_en.add_user_text(translated_input)\n",
    "        \n",
    "        # Set episode done if exit conidion is met. TODO: Better check of input stop\n",
    "        if self.nbr_replies == self.max_replies and len(self.questions) == 0 or user_input.lower().replace(' ','') in self.stop_tokens:\n",
    "            self.episode_done = True\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def act(self):\n",
    "        # Get context\n",
    "        # Get \n",
    "        \n",
    "        if not episode_done:\n",
    "            \n",
    "            # fixa context\n",
    "            # kör igneom model\n",
    "            # strip token\n",
    "            # addera output till convos\n",
    "            # increment self.nbr_replies om modellsvar är ok, annars resetta till 0 och ta fråga från banken\n",
    "            \n",
    "            context = self._get_context()\n",
    "            inputs = self._encode(context)\n",
    "            output_tokens = self.model.generate(**inputs)\n",
    "            reply = self._decode(tokens)\n",
    "            \n",
    "            if self._validate_reply(reply):\n",
    "                self.nbr_replies += 1\n",
    "            else:\n",
    "                # TODO: More tries here? Change context or something\n",
    "                reply = self.questions.pop()\n",
    "                self.replies = 0\n",
    "            \n",
    "            translated_reply = self._en_to_sv()\n",
    "            self.conversation_sv.add_bot_text(translated_reply)\n",
    "            self.conversation_en.add_bot_text(reply)\n",
    "            self.conversation_sv.print_dialogue()\n",
    "            \n",
    "        else:\n",
    "            self.conversation_sv.to_txt()\n",
    "            self.conversation_en.to_txt()\n",
    "            print('Tack för din intervju')   \n",
    "        return\n",
    "    \n",
    "    def _encode(self,text):\n",
    "        return self.tokenizer([text], return_tensors='pt')\n",
    "    \n",
    "    def _decode(self,tokens):\n",
    "        return self._strip_token(self.tokenizer.batch_decode(tokens)[0])\n",
    "        \n",
    "        \n",
    "    def _strip_token(self,line):\n",
    "        # Removes SOS and EOS tokens from blenderbot reply\n",
    "        line = line.replace('<s>','')\n",
    "        line = line.replace('</s>','')\n",
    "        return line\n",
    "    \n",
    "    def _get_context(self):\n",
    "        # Implement this in subclasses\n",
    "        return self.conversation_en.get_dialogue_history()\n",
    "\n",
    "\n",
    "    def _validate_reply(self,answer):\n",
    "        # TODO: \n",
    "        previous_replies = self.conversation_en.get_bot_replies()\n",
    "        \n",
    "        # If answer not in previous_replies ....\n",
    "        if True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def _sv_to_en(self,text):\n",
    "        out = self.translator.translate(text,src='sv',dest='en')\n",
    "        return out.text\n",
    "\n",
    "    def _en_to_sv(self,text):\n",
    "        out = self.translator.translate(text,src='en',dest='sv')\n",
    "        return out.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "patent-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/blenderbot_small-90M were not used when initializing BlenderbotForConditionalGeneration: ['model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layernorm_embedding.bias']\n",
      "- This IS expected if you are initializing BlenderbotForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlenderbotForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlenderbotForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot_small-90M and are newly initialized: ['model.encoder.layer_norm.weight', 'model.encoder.layer_norm.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hej, och välkommen till din intervju. Hur står det till, Alex?\n"
     ]
    }
   ],
   "source": [
    "name ='Alex'\n",
    "job = 'data scientist'\n",
    "mname = 'facebook/blenderbot_small-90M'\n",
    "world = InterviewWorld(name=name, job=job,mname=mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fabulous-country",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f3e87c18b563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Tack det är bra, hur är det med dig?'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-c30fe43638ed>\u001b[0m in \u001b[0;36mchat\u001b[1;34m(self, user_input)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'observe' is not defined"
     ]
    }
   ],
   "source": [
    "user_input = 'Tack det är bra, hur är det med dig?'\n",
    "world.chat(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-invalid",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
