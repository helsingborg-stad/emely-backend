{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integral-break",
   "metadata": {},
   "source": [
    "# Interview experiments for blenderbot\n",
    "\n",
    "In order to get Freja to act as an inteviewer we have to constrain her. Blenderbot(the model) is trained as an open-domain chat bot, which makes her very explorative when it comes to asking questions.\n",
    "\n",
    "#### Approach:\n",
    "- Hard code a set of interview questions\n",
    "- System for determining when it's time to move on to next question\n",
    "- Experiment with the dialogue context she gets at each pass of the conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-toddler",
   "metadata": {},
   "source": [
    "## BlenderConversation is a class for storing the conversation with blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlenderConversation:\n",
    "    \n",
    "    def __init__(self,lang,tokenizer,description='No description'):\n",
    "        self.lang = lang\n",
    "        self.description = description\n",
    "        self.bot_text = []\n",
    "        self.user_text = []\n",
    "        self.user_turn = True\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def reset(self):\n",
    "        self.bot_text = []\n",
    "        self.user_text = []\n",
    "        self.user_turn = True\n",
    "        \n",
    "    def add_user_text(self,text):\n",
    "        if self.user_turn:\n",
    "            self.user_text.append(text)\n",
    "            self.user_turn = False\n",
    "        else:\n",
    "            raise ValueError(\"It's the bot's turn to add a reply to the conversation\")\n",
    "        return\n",
    "    \n",
    "    def add_bot_text(self,text):\n",
    "        if not self.user_turn:\n",
    "            self.bot_text.append(text)\n",
    "            self.user_turn = True\n",
    "        else:\n",
    "            raise ValueError(\"It's the user's turn to add an input to the conversation\")\n",
    "        return\n",
    "    \n",
    "    def pop(self):\n",
    "        if self.user_turn:\n",
    "            self.bot_text.pop()\n",
    "            self.user_turn = False\n",
    "        else:\n",
    "            self.user_text.pop()\n",
    "            self.user_turn = True\n",
    "        return\n",
    "    \n",
    "    def get_bot_replies():\n",
    "        # TODO: Option to return string instead of list?\n",
    "        return self.bot_text\n",
    "    \n",
    "    def get_user_replies():\n",
    "        # TODO: Option to return string instead of list?\n",
    "        return self.user_text\n",
    "        \n",
    "    def get_dialogue_history(self,max_len=100):\n",
    "        # Returns string of the dialogue history with bot and user inputs separated with '\\n'\n",
    "        # max_len set to default 110 as model has max input length 128 and we want some space for new input \n",
    "        history = ''\n",
    "        tokens_left = max_len\n",
    "        if self.user_turn:\n",
    "            # Start backwards from bot_text\n",
    "            for i in reversed(range(len(self.user_text))):\n",
    "                bot_text = self.bot_text[i]\n",
    "                user_text = self.user_text[i]\n",
    "                nbr_tokens = len(self.tokenizer(bot_text)['input_ids'])  + len(self.tokenizer(user_text)['input_ids'])\n",
    "                if  nbr_tokens < tokens_left: # This is not fool proof as the model tokenizer tokenizes differently\n",
    "                    history = user_text + '\\n' + bot_text + '\\n' + history\n",
    "                    tokens_left -= (nbr_tokens + 2)\n",
    "                else:\n",
    "                    break\n",
    "                                \n",
    "        else:\n",
    "            # Start backwards from user_text\n",
    "            history = self.user_text[-1]\n",
    "            tokens_left -= len(self.tokenizer(history)['input_ids'])\n",
    "            for i in reversed(range(len(self.user_text)-1)):\n",
    "                bot_text = self.bot_text[i]\n",
    "                user_text = self.user_text[i]\n",
    "                nbr_tokens = len(bot_text.split()) + len(user_text.split())\n",
    "                if  nbr_tokens < tokens_left: # This is not fool proof as the model tokenizer tokenizes differently\n",
    "                    history = user_text + '\\n' + bot_text + '\\n' + history\n",
    "                    tokens_left -= (nbr_tokens + 2)\n",
    "                else:\n",
    "                    break\n",
    "        return history\n",
    "        \n",
    "    def to_txt(self,file='None'):\n",
    "        # Writes the dialogue to txt file in subdirectory\n",
    "        text = '####################################\\n' + 'Conversation description: ' + self.description + '\\n\\n'\n",
    "        if self.user_turn:\n",
    "            for i in range(len(self.user_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "        else:\n",
    "            for i in range(len(self.bot_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "            text = text + 'User>>> ' + self.user_text[-1]\n",
    "        \n",
    "        if file is None:\n",
    "            if self.lang == 'sv':\n",
    "                file = 'interview_sv.txt'\n",
    "            else:\n",
    "                file = 'interview_en.txt'\n",
    "        \n",
    "        text = text + '\\n\\n'\n",
    "        file_path = '02_interview_output/' + file\n",
    "        with open(file_path,'a') as f:\n",
    "            f.write(text)\n",
    "        return\n",
    "         \n",
    "    \n",
    "    def print_dialogue(self):\n",
    "        # Prints the dialogue \n",
    "        text = ''\n",
    "        if self.user_turn:\n",
    "            for i in range(len(self.user_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "        else:\n",
    "            for i in range(len(self.bot_text)):\n",
    "                text = text + 'User>>> '+ self.user_text[i] + '\\n Bot>>> ' + self.bot_text[i] + '\\n'\n",
    "            text = text + 'User>>> ' + self.user_text[-1]\n",
    "        print(text)\n",
    "        return\n",
    "\n",
    "\n",
    "def strip_token(line):\n",
    "    # Removes SOS and EOS tokens from blenderbot reply\n",
    "    line = line.replace('<s>','')\n",
    "    line = line.replace('</s>','')\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-ribbon",
   "metadata": {},
   "source": [
    "## Class for keeping track of the interview and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minimal-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Om du ska arbeta som car mech så är det bra om du har erfarenhet från YY. Kan du berätta lite om du har sådan erfarenhet?',\n",
       " 'Du har sökt jobbet som car mech. Vad är det som du tycker verkar vara roligt med detta arbetet?',\n",
       " 'Vad är din bästa erfarenhet från dina tidigare arbeten?',\n",
       " 'Är det någonting som du vill fråga om detta arbetet?',\n",
       " 'Vad gör att du skulle passa bra som car mech?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just a place to store questions instead of having txt files for now\n",
    "import random\n",
    "questions = ['Du har sökt jobbet som {}. Vad är det som du tycker verkar vara roligt med detta arbetet?', \n",
    "                       'Om du ska arbeta som {} så är det bra om du har erfarenhet från YY. Kan du berätta lite om du har sådan erfarenhet?',\n",
    "                       'Vad är din bästa erfarenhet från dina tidigare arbeten?',\n",
    "                       'Vad gör att du skulle passa bra som {}?',\n",
    "             'Är det någonting som du vill fråga om detta arbetet?']\n",
    "format_question = [1,1,0,1,0]\n",
    "interview_questions= zip(questions,format_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "manual-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "from torch import no_grad\n",
    "import random\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "class InterviewWorld:\n",
    "    # Class that keeps\n",
    "\n",
    "    def __init__(self, job, name, mname='facebook/blenderbot-1B-distill'):\n",
    "        # TODO: More sophisticated questions/greeting drawn from txt file(?) and formated with name and job\n",
    "        # TODO: init model and tokenizer from file\n",
    "        # TODO: init from opt like dictionary\n",
    "        self.questions = [question.format(job) if format_this else question for (question, format_this) in\n",
    "                          read_questions('interview_questions.txt')]\n",
    "        random.shuffle(self.questions)\n",
    "        self.greeting = 'Hej, och välkommen till din intervju. Hur står det till, {}?'.format(name)\n",
    "        self.context = ''  # TODO, maybe a function that updates this as well\n",
    "\n",
    "        self.job = job\n",
    "        self.human_name = name\n",
    "        self.model = BlenderbotForConditionalGeneration.from_pretrained(mname)  # Try to load on gpu\n",
    "        self.tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n",
    "        self.model_name = mname.replace('facebook/', '')\n",
    "        self.translator = Translator()\n",
    "        self.episode_done = False\n",
    "        self.stop_tokens = ['färdig', 'slut', 'hejdå', 'done']  # TODO: Snyggare lösning\n",
    "        self.max_replies = 2  # Maximum number of replies back and forth for each question\n",
    "        self.nbr_replies = 0\n",
    "\n",
    "        desc = 'InterviewWorld\\t job: {}\\t name: {}\\t model: {}'.format(self.job, self.human_name, self.model_name)\n",
    "        self.conversation_sv = BlenderConversation(lang='sv', tokenizer=self.tokenizer, description=desc)\n",
    "        self.conversation_en = BlenderConversation(lang='en', tokenizer=self.tokenizer, description=desc)\n",
    "\n",
    "        self.greet()\n",
    "\n",
    "    def greet(self):\n",
    "        print(self.greeting)\n",
    "        return\n",
    "\n",
    "    def start(self):\n",
    "        # TODO: Prompt the user to add name and job they're looking for?\n",
    "        return\n",
    "\n",
    "    def reset_conversatoin(self):\n",
    "        self.conversation_sv.reset()\n",
    "        self.conversation_en.reset()\n",
    "        return\n",
    "\n",
    "    def chat(self, user_input):\n",
    "        self.observe(user_input)\n",
    "        self.act()\n",
    "        return\n",
    "\n",
    "    def observe(self, user_input):\n",
    "        # TODO: Add spell check/grammar check here\n",
    "        # Observe the user input, translate and update internal states\n",
    "        # Check if user wants to quit/no questions left --> self.episode_done = True\n",
    "\n",
    "        translated_input = self._sv_to_en(user_input)\n",
    "        self.conversation_sv.add_user_text(user_input)\n",
    "        self.conversation_en.add_user_text(translated_input)\n",
    "\n",
    "        # Set episode done if exit conidion is met. TODO: Better check of input stop\n",
    "        if self.nbr_replies == self.max_replies and len(self.questions) == 0 or user_input.lower().replace(' ',\n",
    "                                                                                                           '') in self.stop_tokens:\n",
    "            self.episode_done = True\n",
    "\n",
    "        return\n",
    "\n",
    "    def act(self):\n",
    "        # Get context\n",
    "        # Get\n",
    "\n",
    "        if not self.episode_done:\n",
    "\n",
    "            # fixa context\n",
    "            # kör igneom model\n",
    "            # strip token\n",
    "            # addera output till convos\n",
    "            # increment self.nbr_replies om modellsvar är ok, annars resetta till 0 och ta fråga från banken\n",
    "\n",
    "            context = self._get_context()\n",
    "            inputs = self._encode(context)\n",
    "            set_trace()\n",
    "            with no_grad():\n",
    "                output_tokens = self.model.generate(**inputs)\n",
    "            reply = self.tokenizer.decode(output_tokens[0],skip_special_tokens=True)\n",
    "\n",
    "            if self._validate_reply(reply):\n",
    "                self.nbr_replies += 1\n",
    "            else:\n",
    "                # TODO: More tries here? Change context or something\n",
    "                reply = self.questions.pop()\n",
    "                self.nbr_replies = 0\n",
    "\n",
    "            translated_reply = self._en_to_sv(reply)\n",
    "            self.conversation_sv.add_bot_text(translated_reply)\n",
    "            self.conversation_en.add_bot_text(reply)\n",
    "            self.conversation_sv.print_dialogue()\n",
    "\n",
    "        else:\n",
    "            self.conversation_sv.to_txt()\n",
    "            self.conversation_en.to_txt()\n",
    "            print('Tack för din intervju')\n",
    "        return\n",
    "\n",
    "    def _encode(self, text):\n",
    "        return self.tokenizer([text], return_tensors='pt')\n",
    "\n",
    "    def _decode(self, tokens):\n",
    "        s = self.tokenizer.decode(tokens,skip_special_tokens=True)\n",
    "        return s\n",
    "\n",
    "\n",
    "    def _get_context(self):\n",
    "        # Implement this in subclasses\n",
    "        return self.conversation_en.get_dialogue_history()\n",
    "\n",
    "    def _validate_reply(self, answer):\n",
    "        # TODO:\n",
    "        previous_replies = self.conversation_en.get_bot_replies()\n",
    "\n",
    "        # If answer not in previous_replies ....\n",
    "        if True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _strip_token(self,line):\n",
    "        # Removes SOS and EOS tokens from blenderbot reply\n",
    "        line = line.replace('<s>', '')\n",
    "        line = line.replace('</s>', '')\n",
    "        return line\n",
    "\n",
    "    # TODO: Change from googletrans to googles official API\n",
    "    def _sv_to_en(self, text):\n",
    "        out = self.translator.translate(text, src='sv', dest='en')\n",
    "        return out.text\n",
    "\n",
    "    def _en_to_sv(self, text):\n",
    "        out = self.translator.translate(text, src='en', dest='sv')\n",
    "        return out.text\n",
    "\n",
    "\n",
    "def read_questions(file_path):\n",
    "    # Reads interview questions from a text file, one question per line. '{}' in place where job should be inserted\n",
    "    root = Path.cwd().parent\n",
    "    file_path2 = root.joinpath('src/chat')\n",
    "    file_path2 = file_path2.joinpath(file_path)\n",
    "    with open(file_path2,'r') as f:\n",
    "        questions = f.readlines()\n",
    "    format_this = [True if '{}' in question else False for question in questions]\n",
    "    return zip(questions,format_this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "patent-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hej, och välkommen till din intervju. Hur står det till, Alex?\n"
     ]
    }
   ],
   "source": [
    "name ='Alex'\n",
    "job = 'data scientist'\n",
    "mname = 'facebook/blenderbot-1B-distill' # options: 'facebook/blenderbot_small-90M' , 'facebook/blenderbot-400M-distill' ,'facebook/blenderbot-3B'\n",
    "world = InterviewWorld(name=name, job=job,mname=mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fabulous-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(87)\u001b[0;36mact\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     85 \u001b[1;33m            \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     86 \u001b[1;33m            \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 87 \u001b[1;33m            \u001b[1;32mwith\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     88 \u001b[1;33m                \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     89 \u001b[1;33m            \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(88)\u001b[0;36mact\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     86 \u001b[1;33m            \u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     87 \u001b[1;33m            \u001b[1;32mwith\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 88 \u001b[1;33m                \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     89 \u001b[1;33m            \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     90 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(89)\u001b[0;36mact\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     87 \u001b[1;33m            \u001b[1;32mwith\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     88 \u001b[1;33m                \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 89 \u001b[1;33m            \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     90 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     91 \u001b[1;33m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_reply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> output_tokens?\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> output_tokens\n",
      "tensor([[   1,  281,  632,  929,  731,   19, 2828,  304,  335, 2099,   21,  855,\n",
      "          366,  304,  929, 1752,   38, 1422,  941, 5018,  335,  271, 3253,   38,\n",
      "            2]])\n",
      "ipdb> output_tokens.shape\n",
      "torch.Size([1, 25])\n",
      "ipdb> self.tokenizer.decode(output_tokens)\n",
      "*** TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'\n",
      "ipdb> self.tokenizer.decode(output_tokens[0])\n",
      "'<s> I am doing well, thank you for asking. How are you doing today? Any fun plans for the weekend?</s>'\n",
      "ipdb> n\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(91)\u001b[0;36mact\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     89 \u001b[1;33m            \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     90 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 91 \u001b[1;33m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_reply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     92 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbr_replies\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     93 \u001b[1;33m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> reply\n",
      "' I am doing well, thank you for asking. How are you doing today? Any fun plans for the weekend?'\n",
      "ipdb> context\n",
      "\"Thanks it's good, how are you?\"\n",
      "ipdb> n\n",
      "TypeError: get_bot_replies() takes 0 positional arguments but 1 was given\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(91)\u001b[0;36mact\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     89 \u001b[1;33m            \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     90 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 91 \u001b[1;33m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_reply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     92 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbr_replies\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     93 \u001b[1;33m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "--Return--\n",
      "None\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(91)\u001b[0;36mact\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     89 \u001b[1;33m            \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     90 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 91 \u001b[1;33m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_reply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     92 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbr_replies\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     93 \u001b[1;33m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "TypeError: get_bot_replies() takes 0 positional arguments but 1 was given\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(53)\u001b[0;36mchat\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     51 \u001b[1;33m    \u001b[1;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     52 \u001b[1;33m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 53 \u001b[1;33m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     54 \u001b[1;33m        \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     55 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "--Return--\n",
      "None\n",
      "> \u001b[1;32m<ipython-input-17-a95faca6262a>\u001b[0m(53)\u001b[0;36mchat\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     51 \u001b[1;33m    \u001b[1;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     52 \u001b[1;33m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m---> 53 \u001b[1;33m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     54 \u001b[1;33m        \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     55 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "TypeError: get_bot_replies() takes 0 positional arguments but 1 was given\n",
      "None\n",
      "> \u001b[1;32m<ipython-input-19-f3e87c18b563>\u001b[0m(2)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      1 \u001b[1;33m\u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Tack det är bra, hur är det med dig?'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 2 \u001b[1;33m\u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f3e87c18b563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Tack det är bra, hur är det med dig?'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\testenv\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'exception'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c_call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\testenv\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_exception\u001b[1;34m(self, frame, arg)\u001b[0m\n\u001b[0;32m    172\u001b[0m                     and arg[0] is StopIteration and arg[2] is None):\n\u001b[0;32m    173\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;31m# Stop at the StopIteration or GeneratorExit exception when the user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;31m# has set stopframe in a generator by issuing a return command, or a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_input = 'Tack det är bra, hur är det med dig?'\n",
    "world.chat(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-order",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
